Student names:

Emilia Zielinska || 300018129
Gabrielle Naubert || 300015305
Robert Zhang || 300077171

----------------------------------------------------------------------------------------------------

PLEASE NOTE:

Stemmer.py is not our work. It is downloaded from
https://tartarus.org/martin/PorterStemmer/python.txt
and is required to be in the submitted zip file in order for the program to work.

----------------------------------------------------------------------------------------------------

How tasks were divided:

For this assignment, we met up online to edit the code together using Visual Studio Code Live
Share. Using this, it allowed us to work on coding simultaneously and work on the tasks together.
None of the tasks were done individually.

----------------------------------------------------------------------------------------------------

Instructions to run the programs:

Please ensure you have the latest version of Python installed. In your terminal:
- run `pip3 install unidecode`
- run `pip3 install sent2vec`
- run `pip3 install rank_bm25`

Note:
- if using Windows, ensure you have have longpaths enabled
  (https://www.reddit.com/r/pytorch/comments/c6cllq/issue_installing_pytorch/ew27hih/)
- if using Windows, ensure you have C++ installed

To run the first test (re-ranking the Results file with BERT), run
`python3 Day1.py <results> <tweets> <queries>`
where <results> is the Results.txt file generated by Assignment 1,
<tweets> is the file containing all the documents, and
<queries> is the file containing all the test queries.
Example: `python3 Day1.py .\Results.txt .\Trec_microblog11.txt .\topics_MB1-49.txt`

Please note that it is expected for Day.py to take over an hour and a half to run.

To run the second test (expanding the query using word2vec), run
`python3 Day2.py <queries> <stopWords> <binary> <tweets>`
where <queries> is the file containing all the test queries,
<stopWords> is the file containing all the stop words,
<binary> is the file of the binary for the pre-trained model, and
<tweets> is the file containing all the documents.
Example: `python3 Day2.py .\topics_MB1-49.txt .\StopWords .\GoogleNews-vectors-negative300.bin
          .\Trec_microblog11.txt`

We recommend to download the binary file from
https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM

----------------------------------------------------------------------------------------------------

Program functionality:

Files -> Day1.py ; Day2.py

Day1.py
main(resultss, tweetss, queriess) ->
    in: text file of results, text file of tweets, text file of queries
    out: text file of re-ranked results

    For each query, the script extracts the 1000 most relevant documents from the results file.
    Through the help of the sent2vec Python package, DistilBERT is used to vectorize the query and
    its relevant tweets. It then uses the spatial package from SciPy to calculate the distance
    between the query vector and each tweet vector, and saves the similarity result as 1 minus the
    distance. The results are formatted into the following:
    <topicId> Q0 <tweetId> <rank> <similarity> PyGER
    This is printed into the Results.txt file.


Day2.py functions -> process() ; retrieve() ; main()
process(s, stopWords) ->
    in: a string, a list of strings
    out: a list

    This is a helper function. It takes in a string and ensures all of the characters are in the
    roman alphabet using unidecode. It then removes punctuation and changes all letters to be in
    lowercase. The string is then split into a list by each space, and all stop words defined by
    stopWords are removed. The resulting list is returned.

retrieve(query, processedTweets, tweets) ->
    in: a list of strings, a list of lists of strings, a list of strings
    out: a dictionary

    This is a helper function. It uses the BM25Okapi Python package to calculate the BM25 score for
    each processed tweet in comparison to the query. The scores are saved into a dictionary where
    the key is the tweet ID and the value is the score. The dictionary is sorted by highest score to
    lowest and then only the top 1000 are kept. The resulting dictionary is returned.

main(queriess, stopWords, binary, i) ->
    in: text file of queries, text file of stop words, binary file of a pre-trained model,
        text file of tweets
    out: text file of results

    At the beginning of the function, each tweet is processed using the process() function and
    stemmed using the Porter Stemmer. Then, for each query, the query is processed using the
    process() function. It is then expanded by adding the top 3 synonyms for each word to the list
    as defined by the word2vec model. The words in the list are then stemmed using the Porter
    Stemmer and the top 1000 results are retrieved using the retrieve() function. The results are
    formatted into the following:
    <topicId> Q0 <tweetId> <rank> <similarity> PyGER
    This is printed into the Results.txt file.

----------------------------------------------------------------------------------------------------

Algorithms, data structures, and optimizations:

    In Day1.py, we start with putting all of the tweets in a dictionary where the key is the tweet
ID and the value is the tweet text for easy retrieval. We then loop through each query and extract
the topic ID and the query text. The query text is saved as the first item in a list called
queryTweets. We then loop through the results file to find the tweet IDs of all the relevant
results from assignment 1, append the tweet text to queryTweets and append the tweet ID to a list
called queryNums. The list queryTweets is then passed on to be vectorized. As queryTweets needs to
be sent to DistilBERT as a list for vectorizing, we cannot keep the IDs in the same list. Instead,
we ensure that the IDs and their corresponding texts are appended in the same order to ensure we can
find the original IDs again. Once the vectorization process is finished, we loop through all of the
tweet vectors and calculate its distance from the query vector using spatial from SciPy. The
similarity score is calculated as 1 minus the distance, and saved into a dictionary where the key is
the tweet's ID. The dictionary is sorted descending by value, and the results are printed to
Results.txt.
    Originally, we tried sending one relevant tweet at a time to DistilBERT to be vectorized.
However, once the code was taking over four hours to run and had not finished the first query, we
realized it would be faster to send as much as possible to DistilBERT at once. Then we came across
the problem of needing around 25GB more RAM in order to vectorize all the tweets at once. We came to
a compromise where we send the relevant tweets for one query at a time, which works with a machine
that has 8GB of RAM, and the program takes around an hour and a half to finish re-ranking all the
results.

    In Day2.py, helper functions are used to keep the code neat. We start off by processing all of
the tweets using process() and stemming them using the Porter Stemmer. This is saved in a list
called processedTweets, and the original tweets are saved in a list called tweets. processedTweets
is a list of lists of strings where each string is a word in a tweet, and each list is a tweet. Once
all of the tweets are processed, we loop through each query and extract the topic ID and the query
text. The query text is then processed using process(), but not stemmed. Before it can be stemmed,
we use word2vec through the gensim package to find the top 3 synonyms for each word. This is done
thanks to the binary of the pre-trained model that we downloaded online. If the word and its
synonyms are not in a list called expandedQuery, they are added to it. If the model does not
recognize word, it is added to the expandedQuery and the KeyError is ignored, meaning no synonyms
for that word are added. After all synonyms have been found, all the words in expandedQuery are
stemmed, and any resulting duplicates are removed. The original tweets, processedTweets, and
expandedQuery are then passed to retrieve() to find their similarity scores using BM25. We decided
upon BM25 instead of TF-IDF as it does not need to calculate the query weight. All of the scores
are added to a dictionary where the key is their corresponding tweet ID. The dictionary is then
sorted descending by value, and only the first 1000 items are kept. It is returned back to the main
function where it gets printed to Results.txt.

----------------------------------------------------------------------------------------------------

First 10 answers for query 3:

3 Q0 32910196598636545 1 18.187920676573096 PyGER
3 Q0 29278582916251649 2 18.187920676573096 PyGER
3 Q0 33711164877701120 3 17.19119076605058 PyGER
3 Q0 32204788955357184 4 17.19119076605058 PyGER
3 Q0 29296574815272960 5 17.19119076605058 PyGER
3 Q0 29615296666931200 6 16.29803016579723 PyGER
3 Q0 29613127372898304 7 16.29803016579723 PyGER
3 Q0 32411439918489600 8 16.204382712002936 PyGER
3 Q0 32387196078006272 9 16.204382712002936 PyGER
3 Q0 32384227123134464 10 16.204382712002936 PyGER

----------------------------------------------------------------------------------------------------

First 10 answers for query 20:

20 Q0 33062378086076416 1 21.093184649630842 PyGER
20 Q0 32269920704135169 2 21.093184649630842 PyGER
20 Q0 29906116062220290 3 20.13778447570271 PyGER
20 Q0 33255149195501568 4 19.870220613307048 PyGER
20 Q0 32944671785226240 5 19.870220613307048 PyGER
20 Q0 33356942797701120 6 19.863622883556445 PyGER
20 Q0 30018392773627905 7 18.932206456964323 PyGER
20 Q0 30012275423186944 8 18.932206456964323 PyGER
20 Q0 31082136219947008 9 18.32745061713637 PyGER
20 Q0 29853985930219520 10 18.32745061713637 PyGER

----------------------------------------------------------------------------------------------------

Final results:

For experiment #1 (Day1.py), we re-ranked the Results.txt file from Assignment 1 using DistilBERT.
    $ ./trec_eval.exe -m map -m P.10 ../Trec_microblog11-qrels.txt ../Results-1.txt
    map                     all     0.0544
    P_10                    all     0.0204
As can be seen, both the MAP and the P@10 score went down significantly from Assignment 1. While
this was unexpected, it is possible for a number of reasons. For example, it could be due to the
pre-set pre-trained DistilBERT model not having been trained on a proper corpus.

For experiment #2 (Day2.py), we expanded the query by adding synonyms and re-retireved the results
using the expanded query.
    $ ./trec_eval.exe -m map -m P.10 ../Trec_microblog11-qrels.txt ../Results-2.txt
    map                     all     0.2324
    P_10                    all     0.3020
Both scores went up from Assignment 1, MAP by 0.0266 and P@10 by 0.0734. This could be because in
addition to adding more words to the query, a different ranking method more suitable to short tweets
had been used. The amount that it increased by was expected, as query expansion should not raise
the score by a lot.
The Results.txt file that has been submitted has been generated by this experiment (Day2.py).
